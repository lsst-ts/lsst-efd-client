{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunked queries: Efficient Data Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with large result sets, fetching all the data at once can lead to excessive memory usage and slower performance. \n",
    "Use \"chunked queries\" to retrieve data in smaller chunks. \n",
    "\n",
    "Chunked queries are handy when working with millions of data points. \n",
    "Rather than requesting the entire result set in one call, we can specify a chunk size to split the data.\n",
    "\n",
    "It's important to note that the optimal chunk size may vary depending on the specific query. Also note that while smaller chunks generally result in faster queries, setting the chunk size too small can introduce overhead by generating many requests to the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lsst_efd_client import EfdClient\n",
    "\n",
    "client = EfdClient(\"usdf_efd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable chunked responses in your query, you can simply set the chunked parameter to `True`. \n",
    "Additionally, you can specify the `chunk_size` parameter, which determines the maximum number of data points in each chunk. \n",
    "By default, the chunk size is set to 1000, but you can adjust it based on your specific requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T16:07:53.137958Z",
     "iopub.status.busy": "2023-07-06T16:07:53.137446Z",
     "iopub.status.idle": "2023-07-06T16:07:53.141417Z",
     "shell.execute_reply": "2023-07-06T16:07:53.140923Z",
     "shell.execute_reply.started": "2023-07-06T16:07:53.137939Z"
    },
    "tags": []
   },
   "source": [
    "Let's consider an example where we need to query the `lsst.sal.MTM1M3.forceActuatorData` topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "await client.get_schema(\"lsst.sal.MTM1M3.forceActuatorData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T16:19:42.348212Z",
     "iopub.status.busy": "2023-07-06T16:19:42.347824Z",
     "iopub.status.idle": "2023-07-06T16:19:42.352456Z",
     "shell.execute_reply": "2023-07-06T16:19:42.351791Z",
     "shell.execute_reply.started": "2023-07-06T16:19:42.348193Z"
    },
    "tags": []
   },
   "source": [
    "\n",
    "This particular topic contains a massive payload, making it inefficient to retrieve all the data using a `SELECT * FROM ...` statement. \n",
    "Instead, it is recommended to select only the fields of interest. \n",
    "This significantly improves the query speed.\n",
    "\n",
    "The topic schema reveals the presence of array fields. \n",
    "In this topic, each element within the array corresponds to a distinct force actuator. \n",
    "Specifically, there are 156 actuators responsible for exerting forces in the `x` direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT /xForce/ \n",
    "    FROM \"lsst.sal.MTM1M3.forceActuatorData\" \n",
    "    WHERE time > now()-6h\"\"\"\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T16:10:04.755017Z",
     "iopub.status.busy": "2023-07-06T16:10:04.754690Z",
     "iopub.status.idle": "2023-07-06T16:10:04.758705Z",
     "shell.execute_reply": "2023-07-06T16:10:04.758191Z",
     "shell.execute_reply.started": "2023-07-06T16:10:04.754995Z"
    },
    "tags": []
   },
   "source": [
    "By implementing chunked queries with the appropriate configuration, we can retrieve a dataframe with millions of data points in less than a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunks = await client._influx_client.query(query, chunked=True, chunk_size=1000)\n",
    "df = pd.concat([chunk async for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
